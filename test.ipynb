{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': ['0', '1', '2', '3', '4'], 'translation': [{'en': 'Source: Project GutenbergAudiobook available here', 'hu': 'Source: mek.oszk.huTranslation: Szenczi MiklósAudiobook available here'}, {'en': 'Pride and Prejudice', 'hu': 'Büszkeség és balítélet'}, {'en': 'Jane Austen', 'hu': 'Jane Austen'}, {'en': 'Chapter 1', 'hu': 'I. KÖNYV 1. FEJEZET'}, {'en': 'It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.', 'hu': 'Általánosan elismert igazság, hogy a legényembernek, ha vagyonos, okvetlenül kell feleség.'}]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Tải tập dữ liệu từ Hugging Face Hub\n",
    "dataset = load_dataset(\"opus_books\",\"en-hu\")\n",
    "\n",
    "# Hiển thị một số mẫu dữ liệu từ tập huấn luyện\n",
    "print(dataset[\"train\"][0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu đã được lưu vào file opus_books_en_hu.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Tải tập dữ liệu từ Hugging Face Hub\n",
    "dataset = load_dataset(\"opus_books\", \"en-hu\")\n",
    "\n",
    "# Chuyển đổi tập dữ liệu thành DataFrame\n",
    "data = dataset['train'][:]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Tách cột 'translation' thành hai cột riêng biệt 'en' và 'hu'\n",
    "df = df.join(pd.json_normalize(df['translation']))\n",
    "df.drop(columns=['translation'], inplace=True)\n",
    "\n",
    "# Lưu DataFrame thành file CSV\n",
    "df.to_csv(\"opus_books_en_hu.csv\", index=False)\n",
    "\n",
    "print(\"Dữ liệu đã được lưu vào file opus_books_en_hu.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "\n",
    "# Đọc dữ liệu từ file CSV\n",
    "data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Chia thành dữ liệu huấn luyện, validation và kiểm tra\n",
    "train_data, test_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)\n",
    "\n",
    "# Khởi tạo tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Định nghĩa lớp Dataset cho dữ liệu\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        en_sentence = self.data.iloc[idx]['en']\n",
    "        hu_sentence = self.data.iloc[idx]['hu']\n",
    "\n",
    "        encoding = self.tokenizer(en_sentence, hu_sentence, \n",
    "                                  return_tensors='pt', \n",
    "                                  max_length=self.max_length, \n",
    "                                  padding='max_length', \n",
    "                                  truncation=True)\n",
    "        \n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Add labels\n",
    "        labels = input_ids.clone()\n",
    "        labels[input_ids == self.tokenizer.pad_token_id] = -100  # Mask token\n",
    "        \n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "\n",
    "# Khởi tạo dataset cho huấn luyện, validation và kiểm tra\n",
    "train_dataset = TranslationDataset(train_data, tokenizer)\n",
    "val_dataset = TranslationDataset(val_data, tokenizer)\n",
    "test_dataset = TranslationDataset(test_data, tokenizer)\n",
    "\n",
    "# Khởi tạo DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# Xây dựng mô hình Transformer\n",
    "class TransformerTranslator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerTranslator, self).__init__()\n",
    "        self.config = BertConfig.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.model = BertModel.from_pretrained('bert-base-multilingual-cased', config=self.config)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, self.config.vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = self.fc(outputs.last_hidden_state)\n",
    "        return logits\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TransformerTranslator().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * input_ids.size(0)\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "            val_loss += loss.item() * input_ids.size(0)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 2)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0) * labels.size(1)\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Đánh giá mô hình trên tập kiểm tra\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "        test_loss += loss.item() * input_ids.size(0)\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 2)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0) * labels.size(1)\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
